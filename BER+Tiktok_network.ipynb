{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShannonBonilla/COMM557_Project/blob/main/BER%2BTiktok_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e509874",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5e509874",
        "outputId": "d4ae5dbc-a217-4792-be14-29ce31d0e4ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bertopic\n",
            "  Downloading bertopic-0.17.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.12/dist-packages (from bertopic) (0.8.40)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from bertopic) (0.5.9.post2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from bertopic) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.12/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.12/dist-packages (from bertopic) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from bertopic) (5.1.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.12/dist-packages (from bertopic) (4.67.1)\n",
            "Requirement already satisfied: llvmlite>0.36.0 in /usr/local/lib/python3.12/dist-packages (from bertopic) (0.43.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.12/dist-packages (from hdbscan>=0.8.29->bertopic) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from hdbscan>=0.8.29->bertopic) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->bertopic) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=4.7.0->bertopic) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly>=4.7.0->bertopic) (25.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.57.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.15.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.12/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.12/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (1.1.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.6.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.10.5)\n",
            "Downloading bertopic-0.17.3-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bertopic\n",
            "Successfully installed bertopic-0.17.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/hdbscan/robust_single_linkage_.py:175: SyntaxWarning: invalid escape sequence '\\{'\n",
            "  $max \\{ core_k(a), core_k(b), 1/\\alpha d(a,b) \\}$.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'final_songs_with_lyrics.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-605172022.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# STEP 1. Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# ==========================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final_songs_with_lyrics.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of unique songs: {df['track_name'].nunique()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'final_songs_with_lyrics.csv'"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# TikTok Song Topic & Network Analysis (FIXED VERSION)\n",
        "# ==========================================================\n",
        "\n",
        "# --- Install dependencies (if running in Colab) ---\n",
        "# !pip install pandas bertopic sentence-transformers hdbscan langdetect networkx matplotlib\n",
        "!pip install langdetect --quiet\n",
        "!pip install bertopic\n",
        "\n",
        "# --- Imports ---\n",
        "import pandas as pd\n",
        "import re\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter, defaultdict\n",
        "from itertools import combinations\n",
        "from langdetect import detect, DetectorFactory\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import hdbscan\n",
        "\n",
        "DetectorFactory.seed = 0  # make results reproducible\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 1. Load data\n",
        "# ==========================================================\n",
        "df = pd.read_csv(\"final_songs_with_lyrics.csv\")\n",
        "\n",
        "print(f\"Number of unique songs: {df['track_name'].nunique()}\")\n",
        "print(f\"Number of unique artists: {df['artist_name'].nunique()}\")\n",
        "\n",
        "audio_features = ['danceability', 'energy', 'loudness', 'tempo', 'duration_ms']\n",
        "print(df[audio_features].describe())\n",
        "\n",
        "# Check missing lyrics\n",
        "df['lyrics_missing'] = df['lyrics'].isnull()\n",
        "df_with_lyrics = df.dropna(subset=['lyrics']).copy()\n",
        "print(f\"Original df size: {df.shape[0]}\")\n",
        "print(f\"df_with_lyrics size: {df_with_lyrics.shape[0]}\")\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 2. Clean and filter lyrics\n",
        "# ==========================================================\n",
        "def detect_language_safe(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "\n",
        "df_with_lyrics['language'] = df_with_lyrics['lyrics'].apply(detect_language_safe)\n",
        "print(df_with_lyrics['language'].value_counts().head())\n",
        "\n",
        "# Remove filler words and noise\n",
        "FILLERS = r\"(oh|yeah|ya|yea|na|la|uh|woo|ooh|ah|ha|hey|baby|girl|boy)\"\n",
        "FILLER_SEQ = re.compile(rf\"\\b(?:{FILLERS})(?:\\s+\\1){{1,}}\\b\", re.IGNORECASE)\n",
        "\n",
        "def clean_lyric(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = re.sub(r\"[\\[\\(\\{].*?[\\]\\)\\}]\", \" \", s)\n",
        "    s = s.lower().replace(\"'\", \"'\")\n",
        "    s = re.sub(r\"[^\\w\\s']\", \" \", s)\n",
        "    s = re.sub(r\"'\", \"\", s)\n",
        "    s = FILLER_SEQ.sub(lambda m: \" \" + \" \".join(sorted(set(m.group(0).split()))) + \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "docs = df_with_lyrics['lyrics'].tolist()\n",
        "clean_docs = [clean_lyric(d) for d in docs if isinstance(d, str)]\n",
        "clean_docs = [d for d in clean_docs if len(d.split()) >= 5]\n",
        "print(f\"Kept {len(clean_docs)} cleaned lyrics\")\n",
        "\n",
        "# ‚úÖ FIX: Filter df_with_lyrics to match cleaned docs length\n",
        "# Precompute cleaned lyrics to avoid redundant cleaning\n",
        "df_with_lyrics['cleaned_text'] = df_with_lyrics['lyrics'].apply(clean_lyric)\n",
        "mask = df_with_lyrics['cleaned_text'].apply(lambda x: len(x.split()) >= 5)\n",
        "df_cleaned = df_with_lyrics[mask].copy()\n",
        "print(f\"df_cleaned size: {len(df_cleaned)} (matches {len(clean_docs)} docs)\")\n",
        "\n",
        "# ‚úÖ Verify consistency\n",
        "assert len(df_cleaned) == len(clean_docs), \"Mismatch between df_cleaned and clean_docs!\"\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 3. Topic modeling with BERTopic\n",
        "# ==========================================================\n",
        "print(\"\\nüîÑ Starting topic modeling...\")\n",
        "\n",
        "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "# ‚úÖ Comprehensive multilingual stopwords\n",
        "comprehensive_stopwords = [\n",
        "    # English\n",
        "    \"you\", \"your\", \"the\", \"and\", \"to\", \"me\", \"it\", \"my\", \"im\", \"on\", \"in\", \"of\", \"for\", \"we\", \"i\", \"a\", \"be\", \"is\", \"are\", \"was\", \"were\",\n",
        "    \"oh\", \"yeah\", \"na\", \"la\", \"uh\", \"mm\", \"mmm\", \"ooh\", \"woah\", \"hey\", \"just\", \"like\", \"got\", \"get\", \"let\", \"ill\", \"youre\", \"ive\", \"dont\", \"cant\", \"aint\",\n",
        "    # Spanish\n",
        "    \"que\", \"de\", \"no\", \"me\", \"te\", \"en\", \"el\", \"lo\", \"yo\", \"tu\", \"mi\", \"y\", \"un\", \"una\", \"con\", \"por\", \"si\", \"se\", \"las\", \"los\", \"del\", \"al\", \"es\", \"son\", \"sta\", \"estoy\",\n",
        "    # Portuguese\n",
        "    \"eu\", \"um\", \"uma\", \"n√£o\", \"do\", \"da\", \"em\", \"no\", \"na\", \"com\", \"pra\", \"t√°\", \"t√¥\", \"pra\", \"por\", \"para\",\n",
        "    # French\n",
        "    \"je\", \"tu\", \"il\", \"elle\", \"nous\", \"vous\", \"ils\", \"elles\", \"un\", \"une\", \"des\", \"le\", \"la\", \"les\", \"et\", \"ou\", \"mais\", \"que\", \"qui\",\n",
        "    # German\n",
        "    \"ich\", \"du\", \"er\", \"sie\", \"wir\", \"ihr\", \"sie\", \"es\", \"der\", \"die\", \"das\", \"den\", \"dem\", \"ein\", \"eine\",\n",
        "    # Common short words to skip\n",
        "    \"way\", \"get\", \"got\", \"make\", \"see\", \"know\"\n",
        "]\n",
        "\n",
        "vectorizer_model = CountVectorizer(\n",
        "    stop_words=comprehensive_stopwords,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=5,\n",
        "    token_pattern=r\"(?u)\\b[a-zA-Z√°√©√≠√≥√∫√±√º√ßƒü≈üƒ±√∂√Ü√ò√Ö√¶√∏√•√Ñ√ñ√ú√ü]{4,}\\b\"  # Skip tokens < 4 chars\n",
        ")\n",
        "\n",
        "hdbscan_model = hdbscan.HDBSCAN(\n",
        "    min_cluster_size=4,\n",
        "    min_samples=1,\n",
        "    metric='euclidean',\n",
        "    cluster_selection_method='eom',\n",
        "    prediction_data=True\n",
        ")\n",
        "\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    nr_topics=20,\n",
        "    calculate_probabilities=False,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "topics, probs = topic_model.fit_transform(clean_docs)\n",
        "topic_info = topic_model.get_topic_info()\n",
        "print(topic_info.head(15))\n",
        "\n",
        "# ‚úÖ FIX: Assign topics to df_cleaned (not df_with_lyrics)\n",
        "assert len(topics) == len(df_cleaned), f\"Mismatch: {len(topics)} topics vs {len(df_cleaned)} rows\"\n",
        "df_cleaned['topic'] = topics\n",
        "\n",
        "# ‚úÖ FIX: Save only once with consistent data\n",
        "df_cleaned.to_csv(\"dataset_with_topics.csv\", index=False)\n",
        "print(f\"‚úÖ Saved dataset_with_topics.csv with {len(df_cleaned)} rows\\n\")\n",
        "\n",
        "# ==========================================================\n",
        "# STEP 4. Build TikTok song network\n",
        "# ==========================================================\n",
        "print(\"üîÑ Building network...\")\n",
        "\n",
        "# ‚úÖ FIX: Check if 'source' column exists, handle gracefully\n",
        "df = pd.read_csv(\"dataset_with_topics.csv\")\n",
        "print(f\"Columns in dataset: {list(df.columns)}\")\n",
        "\n",
        "# Check if source column exists\n",
        "if 'source' in df.columns:\n",
        "    tiktok = df.query(\"source == 'tiktok' & lyrics_missing == False\").copy()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  'source' column not found. Using all data instead.\")\n",
        "    tiktok = df[df['lyrics_missing'] == False].copy()\n",
        "\n",
        "if len(tiktok) == 0:\n",
        "    print(\"‚ö†Ô∏è  No TikTok songs found. Using all available data.\")\n",
        "    tiktok = df.copy()\n",
        "\n",
        "print(f\"TikTok songs: {len(tiktok)}\")\n",
        "\n",
        "tiktok['song_id'] = tiktok.reset_index().index.astype(str)\n",
        "tiktok['topic_id'] = 'T' + tiktok['topic'].astype(str)\n",
        "\n",
        "# --- Bipartite Network ---\n",
        "B = nx.Graph()\n",
        "songs = tiktok['song_id'].unique()\n",
        "topics_ = tiktok['topic_id'].unique()\n",
        "B.add_nodes_from(songs, bipartite=0, node_type='song')\n",
        "B.add_nodes_from(topics_, bipartite=1, node_type='topic')\n",
        "B.add_edges_from(tiktok[['song_id', 'topic_id']].itertuples(index=False))\n",
        "\n",
        "print(f\"Bipartite network: {B.number_of_nodes()} nodes, {B.number_of_edges()} edges\")\n",
        "\n",
        "# --- Song-Song Weighted Network ---\n",
        "song2topics = tiktok.groupby('song_id')['topic_id'].apply(set).to_dict()\n",
        "edges = []\n",
        "\n",
        "for s1, s2 in combinations(song2topics.keys(), 2):\n",
        "    shared = song2topics[s1] & song2topics[s2]\n",
        "    if shared:\n",
        "        edges.append((s1, s2, {'weight': len(shared)}))\n",
        "\n",
        "G = nx.Graph()\n",
        "G.add_edges_from(edges)\n",
        "G.remove_nodes_from(list(nx.isolates(G)))\n",
        "print(f\"Weighted song network: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
        "\n",
        "# --- Community Detection ---\n",
        "from networkx.algorithms import community\n",
        "song_to_community = {}  # Initialize early for visualization\n",
        "\n",
        "if G.number_of_nodes() > 0:\n",
        "    communities = community.greedy_modularity_communities(G, weight='weight')\n",
        "    print(f\"Detected {len(communities)} communities\")\n",
        "\n",
        "    # Print community statistics\n",
        "    for i, comm in enumerate(communities):\n",
        "        print(f\"  Community {i}: {len(comm)} songs\")\n",
        "\n",
        "    # Create community membership mapping (BEFORE visualization)\n",
        "    for i, comm in enumerate(communities):\n",
        "        for song_id in comm:\n",
        "            song_to_community[song_id] = i\n",
        "\n",
        "    tiktok['community'] = tiktok['song_id'].map(song_to_community)\n",
        "\n",
        "    # ‚úÖ Generate community names based on top topics\n",
        "    community_names = {}\n",
        "    for i, comm in enumerate(communities):\n",
        "        comm_songs = tiktok[tiktok['community'] == i]\n",
        "        top_topics = comm_songs['topic_id'].value_counts().head(3)\n",
        "\n",
        "        # Extract topic names for this community\n",
        "        if len(top_topics) > 0:\n",
        "            topic_labels = []\n",
        "            for topic_id in top_topics.index:\n",
        "                # Extract number from 'T0', 'T1', etc.\n",
        "                topic_num = int(topic_id.replace('T', ''))\n",
        "                topic_info = topic_model.get_topic(topic_num)\n",
        "\n",
        "                if topic_info is not None and isinstance(topic_info, list) and len(topic_info) > 0:\n",
        "                    # Get top word from this topic\n",
        "                    top_word = topic_info[0][0]  # (word, score) tuple\n",
        "                    topic_labels.append(f\"{top_word}\")\n",
        "\n",
        "            if topic_labels:\n",
        "                community_name = f\"Community {i}: {', '.join(topic_labels[:3])}\"\n",
        "            else:\n",
        "                community_name = f\"Community {i}\"\n",
        "        else:\n",
        "            community_name = f\"Community {i}\"\n",
        "\n",
        "        community_names[i] = community_name\n",
        "\n",
        "    # Save community analysis to CSV for reference\n",
        "    community_summary = []\n",
        "    for i in range(len(communities)):\n",
        "        comm_songs = tiktok[tiktok['community'] == i]\n",
        "        top_topics = comm_songs['topic_id'].value_counts().head(5)\n",
        "\n",
        "        summary = {\n",
        "            'community_id': i,\n",
        "            'community_name': community_names[i],\n",
        "            'num_songs': len(comm_songs),\n",
        "            'avg_popularity': comm_songs['popularity'].mean() if 'popularity' in tiktok.columns else None,\n",
        "            'top_topics': ', '.join(top_topics.index.tolist()),\n",
        "            'topic_distribution': ', '.join([f\"{t}({c})\" for t, c in top_topics.items()])\n",
        "        }\n",
        "        community_summary.append(summary)\n",
        "\n",
        "    community_df = pd.DataFrame(community_summary)\n",
        "    community_df.to_csv(\"community_analysis.csv\", index=False)\n",
        "    print(\"\\n‚úÖ Community analysis saved to community_analysis.csv\")\n",
        "    print(community_df.to_string())\n",
        "\n",
        "    # ‚úÖ RQ1 ANALYSIS: Link communities to streaming success\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RQ1 ANALYSIS: Community Characteristics & Streaming Success\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Check what streaming metrics are available\n",
        "    streaming_cols = [col for col in tiktok.columns if 'stream' in col.lower() or 'play' in col.lower() or 'view' in col.lower()]\n",
        "    print(f\"\\nüìä Available streaming metrics: {streaming_cols}\")\n",
        "\n",
        "    if len(streaming_cols) > 0:\n",
        "        # Analyze streaming success by community\n",
        "        print(f\"\\nüìà Streaming Success by Community:\")\n",
        "        community_stats = []\n",
        "\n",
        "        for i, comm in enumerate(communities):\n",
        "            comm_songs = tiktok[tiktok['community'] == i]\n",
        "\n",
        "            # Collect stats\n",
        "            stats = {\n",
        "                'community': i,\n",
        "                'num_songs': len(comm),\n",
        "                'avg_popularity': comm_songs['popularity'].mean() if 'popularity' in tiktok.columns else None,\n",
        "            }\n",
        "\n",
        "            # Add streaming metrics if available\n",
        "            for col in streaming_cols:\n",
        "                if col in tiktok.columns:\n",
        "                    stats[f'avg_{col}'] = comm_songs[col].mean()\n",
        "\n",
        "            community_stats.append(stats)\n",
        "\n",
        "            print(f\"\\n  Community {i}:\")\n",
        "            print(f\"    - Songs: {len(comm)}\")\n",
        "            if 'popularity' in tiktok.columns:\n",
        "                print(f\"    - Avg Popularity: {stats['avg_popularity']:.2f}\")\n",
        "            for col in streaming_cols:\n",
        "                if col in tiktok.columns:\n",
        "                    print(f\"    - Avg {col}: {stats[f'avg_{col}']:.2e}\")\n",
        "\n",
        "            # Show top topics in this community\n",
        "            top_topics = comm_songs['topic_id'].value_counts().head(3)\n",
        "            print(f\"    - Top Topics: {', '.join([f'{t}({c})' for t,c in top_topics.items()])}\")\n",
        "\n",
        "        # Rank communities by success\n",
        "        if 'popularity' in tiktok.columns:\n",
        "            ranked = sorted(community_stats, key=lambda x: x['avg_popularity'], reverse=True)\n",
        "            print(f\"\\nüèÜ Communities ranked by average popularity:\")\n",
        "            for rank, comm_stat in enumerate(ranked, 1):\n",
        "                print(f\"  {rank}. Community {comm_stat['community']}: {comm_stat['avg_popularity']:.2f}\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  No streaming success metrics found in dataset\")\n",
        "        print(\"   Available columns:\", list(tiktok.columns)[:10])\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Network has no nodes. Skipping community detection.\")\n",
        "\n",
        "# --- Visualization ---\n",
        "if G.number_of_nodes() > 0:\n",
        "    plt.figure(figsize=(14, 12))\n",
        "    pos = nx.spring_layout(G, seed=42, k=0.5, iterations=50)\n",
        "\n",
        "    # ‚úÖ Color nodes by community\n",
        "    node_colors = []\n",
        "    color_map = plt.cm.Set3(range(len(communities)))\n",
        "\n",
        "    for node in G.nodes():\n",
        "        # Find which community this node belongs to\n",
        "        node_community = song_to_community.get(node, -1)\n",
        "        if node_community >= 0:\n",
        "            node_colors.append(color_map[node_community])\n",
        "        else:\n",
        "            node_colors.append('lightgray')\n",
        "\n",
        "    # Draw with community colors\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=50, alpha=0.8, node_color=node_colors)\n",
        "    nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.5, edge_color='gray')\n",
        "\n",
        "    plt.title(\"TikTok Song Network by Shared Topics\\n(Colored by Community)\",\n",
        "              fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "    # Add legend with community names\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [Patch(facecolor=color_map[i], alpha=0.8,\n",
        "                            label=f'{community_names[i]} (n={len(communities[i])})')\n",
        "                       for i in range(len(communities))]\n",
        "    plt.legend(handles=legend_elements, loc='upper left', fontsize=9, framealpha=0.9)\n",
        "\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"tiktok_song_network.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"‚úÖ Network visualization saved as tiktok_song_network.png\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Cannot visualize: network is empty\")\n",
        "\n",
        "# --- Save Network Outputs ---\n",
        "if G.number_of_nodes() > 0:\n",
        "    nx.write_gexf(G, \"tiktok_weighted_song_network.gexf\")\n",
        "    print(\"‚úÖ Network GEXF file saved as tiktok_weighted_song_network.gexf\")\n",
        "\n",
        "print(\"\\n‚úÖ All steps completed successfully! üéâ\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o1nv_imkpQBI"
      },
      "id": "o1nv_imkpQBI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}