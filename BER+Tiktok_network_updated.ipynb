{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShannonBonilla/COMM557_Project/blob/main/BER%2BTiktok_network_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5e509874",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "5e509874",
        "outputId": "80482083-661e-4127-db81-222106e490f6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "No dataset found. Provide dataset_with_topics_FINAL.csv",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3350797126.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"WARNING: Using fallback dataset without labels: {FALLBACK_PATH} ({len(df)} rows)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No dataset found. Provide dataset_with_topics_FINAL.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Basic sanity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No dataset found. Provide dataset_with_topics_FINAL.csv"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# TikTok Song Topic & Network Analysis (Final Fixed Version)\n",
        "# ==========================================================\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import combinations\n",
        "from networkx.algorithms import community\n",
        "from matplotlib.patches import Patch\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 0. Config\n",
        "# ----------------------------------------------------------\n",
        "LABELED_PATH = \"dataset_with_topics_FINAL.csv\"   # labeled file with topic_number/topic_label\n",
        "FALLBACK_PATH = \"dataset_with_topics.csv\"        # fallback if labeled file not found\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 1. Load data (prefer labeled CSV)\n",
        "# ----------------------------------------------------------\n",
        "if os.path.exists(LABELED_PATH):\n",
        "    df = pd.read_csv(LABELED_PATH)\n",
        "    print(f\"Loaded labeled dataset: {LABELED_PATH} ({len(df)} rows)\")\n",
        "elif os.path.exists(FALLBACK_PATH):\n",
        "    df = pd.read_csv(FALLBACK_PATH)\n",
        "    print(f\"WARNING: Using fallback dataset without labels: {FALLBACK_PATH} ({len(df)} rows)\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"No dataset found. Provide dataset_with_topics_FINAL.csv\")\n",
        "\n",
        "# Basic sanity\n",
        "if \"lyrics_missing\" not in df.columns and \"lyrics\" in df.columns:\n",
        "    df[\"lyrics_missing\"] = df[\"lyrics\"].isnull()\n",
        "\n",
        "# Standardize topic column name\n",
        "if \"topic_number\" in df.columns and \"topic\" not in df.columns:\n",
        "    df.rename(columns={\"topic_number\": \"topic\"}, inplace=True)\n",
        "\n",
        "# Convert topic to integer (handles cases like 0.0)\n",
        "if \"topic\" in df.columns:\n",
        "    df[\"topic\"] = pd.to_numeric(df[\"topic\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    # Drop rows where topic could not be parsed\n",
        "    bad_before = len(df)\n",
        "    df = df[df[\"topic\"].notna()].copy()\n",
        "    df[\"topic\"] = df[\"topic\"].astype(int)\n",
        "    if bad_before - len(df) > 0:\n",
        "        print(f\"Dropped {bad_before - len(df)} rows with invalid topic values\")\n",
        "\n",
        "# Standardize label column (auto-detect)\n",
        "label_col_candidates = [c for c in df.columns if c.lower() in (\"topic_label\", \"topicname\", \"topic_name\", \"label\")]\n",
        "if len(label_col_candidates) > 0 and \"topic_name\" not in df.columns:\n",
        "    df.rename(columns={label_col_candidates[0]: \"topic_name\"}, inplace=True)\n",
        "\n",
        "# Drop BERTopic outliers if present\n",
        "if \"topic\" in df.columns:\n",
        "    before = len(df)\n",
        "    df = df[df[\"topic\"] != -1].copy()\n",
        "    print(f\"Dropped {before - len(df)} rows with topic = -1\")\n",
        "\n",
        "# If still no human-readable labels, create placeholders\n",
        "if \"topic_name\" not in df.columns:\n",
        "    print(\"No topic label column detected â€” creating placeholder labels.\")\n",
        "    df[\"topic_name\"] = \"Topic \" + df[\"topic\"].astype(str)\n",
        "\n",
        "# Build topic id -> label mapping (most frequent label per topic)\n",
        "topic_label_map = (\n",
        "    df[[\"topic\", \"topic_name\"]]\n",
        "    .dropna()\n",
        "    .groupby(\"topic\")[\"topic_name\"]\n",
        "    .agg(lambda s: s.value_counts().idxmax())\n",
        "    .to_dict()\n",
        ")\n",
        "print(f\"Topic label map created for {len(topic_label_map)} topics\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 2. Subset to TikTok\n",
        "# ----------------------------------------------------------\n",
        "if \"source\" in df.columns:\n",
        "    tiktok = df.query(\"source == 'tiktok' and lyrics_missing == False\").copy()\n",
        "else:\n",
        "    print(\"No 'source' column found. Using all rows with lyrics.\")\n",
        "    tiktok = df[df.get(\"lyrics_missing\", False) == False].copy()\n",
        "\n",
        "if len(tiktok) == 0:\n",
        "    print(\"No TikTok subset found. Using all data.\")\n",
        "    tiktok = df.copy()\n",
        "\n",
        "print(f\"TikTok subset size: {len(tiktok)}\")\n",
        "\n",
        "# Create stable ids\n",
        "tiktok[\"song_id\"] = tiktok.reset_index().index.astype(str)\n",
        "tiktok[\"topic_id\"] = \"T\" + tiktok[\"topic\"].astype(int).astype(str)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 3. Build song-song network by shared topic\n",
        "# ----------------------------------------------------------\n",
        "song2topics = tiktok.groupby(\"song_id\")[\"topic_id\"].apply(set).to_dict()\n",
        "edges = []\n",
        "for s1, s2 in combinations(song2topics.keys(), 2):\n",
        "    shared = song2topics[s1] & song2topics[s2]\n",
        "    if shared:\n",
        "        edges.append((s1, s2, {\"weight\": len(shared)}))\n",
        "\n",
        "G = nx.Graph()\n",
        "G.add_edges_from(edges)\n",
        "G.remove_nodes_from(list(nx.isolates(G)))\n",
        "print(f\"Weighted song network: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 4. Community detection and naming\n",
        "# ----------------------------------------------------------\n",
        "if G.number_of_nodes() > 0:\n",
        "    comms = community.greedy_modularity_communities(G, weight=\"weight\")\n",
        "    print(f\"Detected {len(comms)} communities\")\n",
        "\n",
        "    # Map song -> community index (0-based internal)\n",
        "    song_to_comm0 = {}\n",
        "    for i, comm in enumerate(comms):\n",
        "        for s in comm:\n",
        "            song_to_comm0[s] = i\n",
        "    tiktok[\"community0\"] = tiktok[\"song_id\"].map(song_to_comm0)\n",
        "\n",
        "    # Build display names with 1-based ids and human-readable topic labels\n",
        "    community_names = {}  # 1-based id -> display name\n",
        "    for i, comm in enumerate(comms, start=1):\n",
        "        comm_rows = tiktok[tiktok[\"community0\"] == (i - 1)]\n",
        "        top_topics = comm_rows[\"topic_id\"].value_counts().head(3)\n",
        "        labels = []\n",
        "        for tid in top_topics.index:\n",
        "            # tid like \"T3\" -> 3\n",
        "            tnum = int(str(tid).replace(\"T\", \"\"))\n",
        "            label = topic_label_map.get(tnum, f\"Topic {tnum}\")\n",
        "            labels.append(label)\n",
        "        name = \", \".join(labels) if labels else \"Mixed Themes\"\n",
        "        community_names[i] = f\"Community {i}: {name}\"\n",
        "\n",
        "    print(\"\\nCommunity names:\")\n",
        "    for i in sorted(community_names):\n",
        "        print(f\"- {community_names[i]}\")\n",
        "else:\n",
        "    comms = []\n",
        "    community_names = {}\n",
        "    print(\"No communities detected; skipping naming.\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 5. Visualization (legend shows top-8 largest communities)\n",
        "# ----------------------------------------------------------\n",
        "if G.number_of_nodes() > 0:\n",
        "    plt.figure(figsize=(14, 12))\n",
        "    pos = nx.spring_layout(G, seed=42, k=0.5, iterations=50)\n",
        "\n",
        "    # Color by internal 0-based community id\n",
        "    color_map = plt.cm.Set3(range(len(comms))) if len(comms) > 0 else None\n",
        "    node_colors = [color_map[song_to_comm0.get(n, 0)] for n in G.nodes()] if color_map is not None else \"lightgray\"\n",
        "\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=50, node_color=node_colors, alpha=0.8)\n",
        "    nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.5, edge_color=\"gray\")\n",
        "\n",
        "    plt.title(\"TikTok Song Network by Shared Lyrical Themes\", fontsize=16)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # Legend for top-8 largest communities (1-based display ids)\n",
        "    if len(comms) > 0:\n",
        "        sizes = {i + 1: len(comms[i]) for i in range(len(comms))}\n",
        "        largest = sorted(sizes, key=sizes.get, reverse=True)[:8]\n",
        "        legend_elements = [\n",
        "            Patch(facecolor=color_map[i - 1], label=f\"{community_names[i]} (n={sizes[i]})\", alpha=0.8)\n",
        "            for i in largest\n",
        "        ]\n",
        "        plt.legend(handles=legend_elements, loc=\"upper left\", fontsize=9, framealpha=0.9, title=\"Lyrical Communities\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"tiktok_song_network.png\", dpi=300, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "    # Export GEXF for Gephi\n",
        "    nx.write_gexf(G, \"tiktok_weighted_song_network.gexf\")\n",
        "    print(\"Exported: tiktok_weighted_song_network.gexf and tiktok_song_network.png\")\n",
        "else:\n",
        "    print(\"Network is empty. Visualization skipped.\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Step 6. RQ1 regression placeholders (run once chart/streaming columns are present)\n",
        "# ----------------------------------------------------------\n",
        "print(\"\\nRegression preparation...\")\n",
        "\n",
        "df_reg = df.copy()\n",
        "chart_vars = [c for c in df_reg.columns if c.lower() in (\"peak_rank\", \"weeks_on_chart\")]\n",
        "stream_vars = [c for c in df_reg.columns if (\"stream\" in c.lower()) or (\"play\" in c.lower()) or (\"view\" in c.lower())]\n",
        "\n",
        "if not chart_vars and not stream_vars:\n",
        "    print(\"No chart or streaming variables found yet. Add them and re-run this section.\")\n",
        "else:\n",
        "    print(f\"Chart vars: {chart_vars}\")\n",
        "    print(f\"Streaming vars: {stream_vars}\")\n",
        "\n",
        "    # Predictors: topic one-hot + basic audio features\n",
        "    needed = [\"topic\", \"danceability\", \"energy\", \"loudness\", \"tempo\"]\n",
        "    for col in needed:\n",
        "        if col not in df_reg.columns:\n",
        "            raise ValueError(f\"Missing required column for regression: {col}\")\n",
        "\n",
        "    # Ensure topic is integer for encoding\n",
        "    df_reg[\"topic\"] = pd.to_numeric(df_reg[\"topic\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    df_reg = df_reg[df_reg[\"topic\"].notna()].copy()\n",
        "    df_reg[\"topic\"] = df_reg[\"topic\"].astype(int)\n",
        "\n",
        "    X = df_reg[needed].dropna()\n",
        "    enc = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "    topic_encoded = enc.fit_transform(X[[\"topic\"]])\n",
        "    topic_cols = [f\"topic_{t}\" for t in enc.categories_[0]]\n",
        "    topic_df = pd.DataFrame(topic_encoded, columns=topic_cols, index=X.index)\n",
        "    X = pd.concat([X.drop(columns=[\"topic\"]), topic_df], axis=1)\n",
        "\n",
        "    # Run OLS for each available dependent variable\n",
        "    for target in chart_vars + stream_vars:\n",
        "        if target not in df_reg.columns:\n",
        "            continue\n",
        "        y = df_reg.loc[X.index, target]\n",
        "        y = pd.to_numeric(y, errors=\"coerce\")\n",
        "        mask = ~y.isna()\n",
        "        if mask.sum() < 30:\n",
        "            print(f\"Not enough rows for {target} (n={mask.sum()}). Skipping.\")\n",
        "            continue\n",
        "        X_ = sm.add_constant(X.loc[mask])\n",
        "        y_ = y.loc[mask]\n",
        "        model = sm.OLS(y_, X_).fit()\n",
        "        print(f\"\\nRegression on {target}:\")\n",
        "        print(model.summary().tables[1])\n",
        "\n",
        "print(\"\\nDone.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o1nv_imkpQBI"
      },
      "id": "o1nv_imkpQBI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}