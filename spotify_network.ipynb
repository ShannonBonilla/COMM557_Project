import pandas as pd, numpy as np, networkx as nx
from networkx.algorithms import community
from pathlib import Path

DATA = Path("data"); OUT = Path("outputs"); OUT.mkdir(exist_ok=True)

def norm(df):
    df.columns = [c.strip().lower().replace(" ", "_") for c in df.columns]
    return df

def keyify(x): return (x or "").strip().lower()

songs = norm(pd.read_csv(DATA/"final_songs_with_lyrics.csv"))
topics = norm(pd.read_csv(DATA/"dataset_with_topics.csv"))   # exported from .numbers

for df in (songs, topics):
    if "title" not in df.columns and "track_name" in df.columns:
        df.rename(columns={"track_name":"title"}, inplace=True)
    if "artist" not in df.columns and "artist_name" in df.columns:
        df.rename(columns={"artist_name":"artist"}, inplace=True)
    df["key"] = df["title"].map(keyify) + " — " + df["artist"].map(keyify)

topic_col = "topic" if "topic" in topics.columns else ("dominant_topic" if "dominant_topic" in topics.columns else None)
assert topic_col is not None, "No 'topic' or 'dominant_topic' column in dataset_with_topics.csv"

# Load TikTok & Spotify chart metadata
def load_concat(pattern):
    dfs=[]
    for p in sorted(DATA.glob(pattern)):
        df = norm(pd.read_csv(p))
        if "title" not in df.columns and "track_name" in df.columns:
            df.rename(columns={"track_name":"title"}, inplace=True)
        if "artist" not in df.columns and "artist_name" in df.columns:
            df.rename(columns={"artist_name":"artist"}, inplace=True)
        df["key"] = df["title"].map(keyify) + " — " + df["artist"].map(keyify)
        dfs.append(df)
    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

tiktok_all  = load_concat("TikTok_songs_*.csv")
spotify_all = load_concat("spotify_top_charts_*.csv")

tiktok_keys = set(tiktok_all["key"]) if not tiktok_all.empty else set()
spotify_keys = set(spotify_all["key"]) if not spotify_all.empty else set()

spotify_only_keys = spotify_keys - tiktok_keys

# Build master song meta
meta = songs[["key","title","artist"]].drop_duplicates().copy()
meta["on_spotify_chart"] = meta["key"].isin(spotify_keys).astype(int)
meta["on_tiktok_list"]  = meta["key"].isin(tiktok_keys).astype(int)
meta["spotify_only"]    = meta["key"].isin(spotify_only_keys).astype(int)

# simple longevity proxy
if "weeks_on_chart" in spotify_all.columns:
    weeks = spotify_all.groupby("key")["weeks_on_chart"].max()
elif "date" in spotify_all.columns:
    weeks = spotify_all.groupby("key")["date"].nunique()  # count distinct chart weeks
else:
    weeks = pd.Series(dtype=float)
meta = meta.merge(weeks.rename("weeks_on_chart"), on="key", how="left").fillna({"weeks_on_chart":0})

#  Merge topics; filter to Spotify-only
topic_df = topics[["key","title","artist", topic_col]].rename(columns={topic_col:"topic"})
df = topic_df.merge(meta, on=["key","title","artist"], how="left")
df = df[df["spotify_only"] == 1].dropna(subset=["topic"]).copy()

print(f"Spotify-only songs with topics: {df['key'].nunique()}")

# Build bipartite
B = nx.Graph()
for _, r in df.iterrows():
    s = f"{r['title']} — {r['artist']}"
    t = f"Topic_{int(r['topic'])}"
    B.add_node(s, bipartite=0, kind="song",
               spotify_only=int(r["spotify_only"]),
               weeks=float(r["weeks_on_chart"]))
    B.add_node(t, bipartite=1, kind="topic")
    B.add_edge(s, t, weight=1.0)

songs_left = [n for n, d in B.nodes(data=True) if d.get("bipartite")==0]
G = nx.bipartite.weighted_projected_graph(B, songs_left)

print(f"Song–song projected graph → nodes: {G.number_of_nodes()}, edges: {G.number_of_edges()}")

# Community detection
try:
    comms = community.louvain_communities(G, seed=42)
except Exception:
    comms = community.greedy_modularity_communities(G, weight="weight")

song2comm = {node:i for i,c in enumerate(comms) for node in c}

# Outputs
song_nodes = pd.DataFrame({
    "song_node": list(G.nodes())
})
song_nodes["title"]  = song_nodes["song_node"].str.split(" — ").str[0]
song_nodes["artist"] = song_nodes["song_node"].str.split(" — ").str[1]
song_nodes["community"] = song_nodes["song_node"].map(song2comm)

# reattach keys & metrics
song_level = (song_nodes
              .merge(meta, on=["title","artist"], how="left")
              [["title","artist","key","spotify_only","weeks_on_chart","community"]]
              .drop_duplicates())

song_level.to_csv(DATA/"spotify_songs_with_communities.csv", index=False)

summary = (song_level.groupby("community")
           .agg(n_songs=("key","count"),
                avg_weeks=("weeks_on_chart","mean"))
           .reset_index()
           .sort_values(["n_songs","avg_weeks"], ascending=False))

summary.to_csv(DATA/"spotify_community_summary.csv", index=False)
nx.to_pandas_edgelist(G).to_csv(OUT/"spotify_song_song_edges.csv", index=False)

print("Top communities (by size):\n", summary.head(10))

nx.write_gexf(G, str(OUT/"spotify_weighted_song_network.gexf"))

print("\nSaved:")
print("- data/spotify_songs_with_communities.csv")
print("- data/spotify_community_summary.csv")
print("- outputs/spotify_song_song_edges.csv")
print("- outputs/spotify_weighted_song_network.gexf")
